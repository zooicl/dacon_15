{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T15:11:57.756088Z",
     "start_time": "2020-02-09T15:11:56.996920Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiden/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import multiprocessing # 여러 개의 일꾼 (cpu)들에게 작업을 분산시키는 역할\n",
    "from multiprocessing import Pool \n",
    "from functools import partial # 함수가 받는 인자들 중 몇개를 고정 시켜서 새롭게 파생된 함수를 형성하는 역할\n",
    "from data_loader import data_loader_v2 # 자체적으로 만든 data loader version 2.0 ([데이콘 15회 대회] 데이터 설명 및 데이터 불러오기 영상 참조)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib # 모델을 저장하고 불러오는 역\n",
    "from datetime import datetime\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from tools import eval_summary, save_feature_importance, merge_preds, report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T15:12:02.557458Z",
     "start_time": "2020-02-09T15:11:57.757308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110    2640\n",
       "17     2310\n",
       "114    2200\n",
       "118    2200\n",
       "117    2090\n",
       "       ... \n",
       "101     110\n",
       "145     110\n",
       "37      110\n",
       "100     110\n",
       "191     110\n",
       "Name: label, Length: 198, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folder = 'data/train/'\n",
    "test_folder = 'data/test/'\n",
    "train_label_path = 'data/train_label.csv'\n",
    "\n",
    "train_list = os.listdir(train_folder)\n",
    "test_list = os.listdir(test_folder)\n",
    "train_label = pd.read_csv(train_label_path, index_col=0)\n",
    "\n",
    "num_class = len(train_label['label'].unique())\n",
    "\n",
    "# 모든 csv 파일의 상태_B로 변화는 시점이 같다라고 가정\n",
    "# 하지만, 개별 csv파일의 상태_B로 변화는 시점은 상이할 수 있음\n",
    "def data_loader_all_v2(func, files, folder='', train_label=None, event_time=10, nrows=60):   \n",
    "    func_fixed = partial(func, folder=folder, train_label=train_label, event_time=event_time, nrows=nrows)     \n",
    "    if __name__ == '__main__':\n",
    "        pool = Pool(processes=multiprocessing.cpu_count()-2) \n",
    "        df_list = list(pool.imap(func_fixed, files)) \n",
    "        pool.close()\n",
    "        pool.join()        \n",
    "    combined_df = pd.concat(df_list)    \n",
    "    return combined_df\n",
    "\n",
    "# event_time = 10\n",
    "# nrows = 30\n",
    "# train = data_loader_all_v2(data_loader_v2, train_list, folder=train_folder, train_label=train_label, \n",
    "#                            event_time=event_time, nrows=nrows)\n",
    "# print(train.shape)\n",
    "# joblib.dump(train, 'data/df_train_{}_{}.pkl'.format(event_time, nrows))\n",
    "\n",
    "train = joblib.load('data/df_train_10_120.pkl').reset_index()\n",
    "\n",
    "# event_time = 10\n",
    "# nrows = 200\n",
    "# test = data_loader_all_v2(data_loader_v2, test_list, folder=test_folder, train_label=None, event_time=event_time, nrows=nrows)\n",
    "# print(test.shape)\n",
    "# joblib.dump(train, 'data/df_test_{}_{}.pkl'.format(event_time, nrows))\n",
    "\n",
    "test = joblib.load('data/df_test_10.pkl')\n",
    "\n",
    "fea_cols = [c for c in train.columns if c[0] == 'V']\n",
    "len(fea_cols)\n",
    "\n",
    "zero_cols = joblib.load('zero_cols.bin')\n",
    "fea_cols = [c for c in fea_cols if c not in zero_cols]\n",
    "\n",
    "# use_cols = joblib.load('use_cols.bin')\n",
    "# fea_cols = use_cols[:10]\n",
    "\n",
    "# fea_cols = zero_cols\n",
    "\n",
    "len(fea_cols)\n",
    "\n",
    "train['label'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T15:12:02.562108Z",
     "start_time": "2020-02-09T15:12:02.558551Z"
    }
   },
   "outputs": [],
   "source": [
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import k_categorical\n",
    "from ngboost.learners import default_tree_learner\n",
    "\n",
    "\n",
    "model_ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "print('model_ts', model_ts)\n",
    "\n",
    "\n",
    "num_round = 1000\n",
    "print('num_round:', num_round)\n",
    "\n",
    "submit_csv = []\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=81511991154 % 2**32-1, shuffle=True)\n",
    "\n",
    "cv = 0\n",
    "for train_index, valid_index in tqdm_notebook(skf.split(train.index, train['label'].values), total=n_splits, desc = 'CV'):\n",
    "    \n",
    "    X_train, X_test = train.loc[train_index, fea_cols], train.loc[valid_index, fea_cols] \n",
    "    y_train, y_test = train.loc[train_index,'label'], train.loc[valid_index, 'label']    \n",
    "    \n",
    "    print(X_train.shape, X_test.shape)\n",
    "#     print(y_train.value_counts(dropna=False))\n",
    "#     print(y_test.value_counts(dropna=False))\n",
    "\n",
    "    model = NGBClassifier(Dist=k_categorical(198),     \n",
    "                        natural_gradient=True,\n",
    "                        n_estimators=500,\n",
    "                        learning_rate=0.01,\n",
    "                        minibatch_frac=0.5,\n",
    "                        verbose=True,\n",
    "                        verbose_eval=50,\n",
    "                        tol=0.0001,) \n",
    "    \n",
    "    model = model.fit(X_train.values, y_train.values, X_test.values, y_test.values, early_stopping_rounds=200)\n",
    "\n",
    "\n",
    "#     model_tag ='{}ngb_{}_{}_{}'.format(model_ts, cv, \n",
    "#                                  evals_result['valid_1']['mlogloss'][model.best_iteration-1],\n",
    "#                                  evals_result['training']['mlogloss'][model.best_iteration-1]\n",
    "#                                 )\n",
    "    \n",
    "    model_tag ='{}ngb_{}'.format(model_ts, cv\n",
    "                                )\n",
    "\n",
    "    print(model_tag)\n",
    "    joblib.dump(model, 'model/{}.model'.format(model_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-08T13:05:30.614Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(test[fea_cols])\n",
    "\n",
    "submission = pd.DataFrame(data=pred)\n",
    "submission.index = test.index\n",
    "submission.index.name = 'id'\n",
    "submission = submission.sort_index()\n",
    "submission = submission.groupby('id').mean()\n",
    "\n",
    "csv_path = 'submit/{}.csv'.format(model_tag)\n",
    "submit_csv.append(csv_path)\n",
    "submission.to_csv(csv_path, index=True) \n",
    "\n",
    "print(submission.sum(axis=1))\n",
    "print(submission)\n",
    "cv += 1\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T12:55:45.240676Z",
     "start_time": "2020-02-07T12:46:53.533Z"
    }
   },
   "outputs": [],
   "source": [
    "# submissions = [\n",
    "# 'submit/20200201T193822_0.42165222105307115_0.03657010393259738.csv',\n",
    "# 'submit/20200201T183544_0.4354487978488266_0.04354968619883053.csv',\n",
    "# 'submit/20200201T173725_0.423066834354457_0.03614391993976106.csv',\n",
    "# 'submit/20200201T163343_0.4287587567261741_0.042392138498467166.csv',\n",
    "# 'submit/20200201T153529_0.42326652930531944_0.04098269988118284.csv',\n",
    "# 'submit/20200201T143616_0.4258237823312355_0.04651153387555587.csv',\n",
    "# 'submit/20200201T134047_0.43147156765580946_0.043682031170534714.csv',\n",
    "# 'submit/20200201T125939_0.42958065644660504_0.040012625819045466.csv',\n",
    "# 'submit/20200201T120739_0.42070899280425217_0.03477196302416032.csv',\n",
    "# 'submit/20200201T110556_0.4181683365658109_0.043843902710973416.csv',\n",
    "# ]\n",
    "\n",
    "\n",
    "dfs = [pd.read_csv(s) for s in submit_csv]\n",
    "\n",
    "df_submit = pd.concat(dfs)\n",
    "\n",
    "df_submit = df_submit.groupby('id').mean()\n",
    "\n",
    "df_submit.to_csv('submit/{}_e{}.csv'.format(model_ts, n_splits), index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T12:55:45.241249Z",
     "start_time": "2020-02-07T12:46:54.419Z"
    }
   },
   "outputs": [],
   "source": [
    "df_submit.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T01:41:26.497335Z",
     "start_time": "2020-02-06T01:01:04.458Z"
    }
   },
   "outputs": [],
   "source": [
    "importance_type = 'split'\n",
    "impt_dict = {k:v for k, v in zip(fea_cols, model.feature_importance(importance_type=importance_type))}\n",
    "# sorted(impt_dict.items(), key=(lambda x:x[1]), reverse=True)\n",
    "# zero_cols = []\n",
    "for k, s in sorted(impt_dict.items(), key=(lambda x:x[1]), reverse=False):\n",
    "    if s == 0:\n",
    "        zero_cols.append(k)\n",
    "# joblib.dump(impt_dict, f'model/{file_name}_{importance_type}.pkl')\n",
    "\n",
    "len(zero_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T01:41:26.498127Z",
     "start_time": "2020-02-06T01:01:04.460Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(zero_cols, 'zero_cols.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T01:41:26.498908Z",
     "start_time": "2020-02-06T01:01:04.462Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = joblib.load('model/20200130T221520_2.4393985000913667_0.07225009557115544.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T01:41:26.499677Z",
     "start_time": "2020-02-06T01:01:04.464Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(test)\n",
    "\n",
    "# submission = pd.DataFrame(data=pred)\n",
    "# submission.index = test.index\n",
    "# submission.index.name = 'id'\n",
    "# submission = submission.sort_index()\n",
    "# submission = submission.groupby('id').mean()\n",
    "\n",
    "# submission.to_csv('submit/{}.csv'.format(model_tag), index=True) \n",
    "# model_tag\n",
    "\n",
    "# submission.sum(axis=1)\n",
    "\n",
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
