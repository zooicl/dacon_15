{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T07:11:28.440261Z",
     "start_time": "2020-02-06T07:11:28.427137Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# In[1]:\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib \n",
    "import os\n",
    "import glob\n",
    "from konlpy.tag import Mecab\n",
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import gc\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import json\n",
    "from typing import NamedTuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "print(torch.__version__)\n",
    "# from tools import eval_summary, save_feature_importance, merge_preds\n",
    "from tools import EarlyStopping\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:18:44.873307Z",
     "start_time": "2020-02-06T06:18:44.870999Z"
    }
   },
   "outputs": [],
   "source": [
    "train_folder = 'data/train/'\n",
    "test_folder = 'data/test/'\n",
    "train_label_path = 'data/train_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:18:44.883897Z",
     "start_time": "2020-02-06T06:18:44.874722Z"
    }
   },
   "outputs": [],
   "source": [
    "train_list = os.listdir(train_folder)\n",
    "test_list = os.listdir(test_folder)\n",
    "train_label = pd.read_csv(train_label_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:18:44.887419Z",
     "start_time": "2020-02-06T06:18:44.885076Z"
    }
   },
   "outputs": [],
   "source": [
    "num_class = len(train_label['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:18:44.893228Z",
     "start_time": "2020-02-06T06:18:44.888487Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모든 csv 파일의 상태_B로 변화는 시점이 같다라고 가정\n",
    "# 하지만, 개별 csv파일의 상태_B로 변화는 시점은 상이할 수 있음\n",
    "def data_loader_all_v2(func, files, folder='', train_label=None, event_time=10, nrows=60):   \n",
    "    func_fixed = partial(func, folder=folder, train_label=train_label, event_time=event_time, nrows=nrows)     \n",
    "    if __name__ == '__main__':\n",
    "        pool = Pool(processes=multiprocessing.cpu_count()) \n",
    "        df_list = list(pool.imap(func_fixed, files)) \n",
    "        pool.close()\n",
    "        pool.join()        \n",
    "    combined_df = pd.concat(df_list)    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:18:48.374712Z",
     "start_time": "2020-02-06T06:18:48.372775Z"
    }
   },
   "outputs": [],
   "source": [
    "# train = data_loader_all_v2(data_loader_v2, train_list, folder=train_folder, train_label=train_label, \n",
    "#                            event_time=10, nrows=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:18:52.193575Z",
     "start_time": "2020-02-06T06:18:49.753827Z"
    }
   },
   "outputs": [],
   "source": [
    "train = joblib.load('data/df_train_10_60.pkl').reset_index()\n",
    "test = joblib.load('data/df_test_10.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:57:43.936215Z",
     "start_time": "2020-02-06T06:57:43.931029Z"
    }
   },
   "outputs": [],
   "source": [
    "y_cols = 'label'\n",
    "fea_cols = [c for c in train.columns if c[0] == 'V']\n",
    "len(fea_cols), y_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:58:02.430980Z",
     "start_time": "2020-02-06T06:57:44.209428Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = joblib.load('scaler_20200129T135731.bin')\n",
    "\n",
    "train[fea_cols] = scaler.transform(train[fea_cols].values)\n",
    "test[fea_cols] = scaler.transform(test[fea_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:58:02.536705Z",
     "start_time": "2020-02-06T06:58:02.431934Z"
    }
   },
   "outputs": [],
   "source": [
    "zero_cols = joblib.load('zero_cols.bin')\n",
    "fea_cols = [c for c in fea_cols if c not in zero_cols]\n",
    "input_size = len(fea_cols)\n",
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:20:51.305994Z",
     "start_time": "2020-02-06T06:20:49.625292Z"
    }
   },
   "outputs": [],
   "source": [
    "# for c in fea_cols:\n",
    "#     if len(train[c].unique()) == 1:\n",
    "#         print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:24:26.925339Z",
     "start_time": "2020-02-06T06:24:26.902174Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:33:47.318267Z",
     "start_time": "2020-02-06T06:33:47.299779Z"
    }
   },
   "outputs": [],
   "source": [
    "df_label = pd.read_csv('data/train_label.csv', index_col=0)\n",
    "df_label\n",
    "\n",
    "id2label = df_label.to_dict()['label']\n",
    "id2label\n",
    "\n",
    "label2id = {}\n",
    "for k, v in id2label.items():\n",
    "    if v in label2id.keys():\n",
    "        label2id[v].add(k)\n",
    "    else:\n",
    "        label2id[v] = set()\n",
    "        label2id[v].add(k)\n",
    "\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:26:27.348670Z",
     "start_time": "2020-02-06T06:26:27.337358Z"
    }
   },
   "outputs": [],
   "source": [
    "fea_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T06:30:58.957503Z",
     "start_time": "2020-02-06T06:30:58.955225Z"
    }
   },
   "outputs": [],
   "source": [
    "x_dict = {}\n",
    "\n",
    "for i in train['index'].unique():\n",
    "    x_dict[i] = [np.stack(train[train['index'] == i][fea_cols].values)]\n",
    "\n",
    "df_seq = pd.DataFrame.from_dict(x_dict, orient='index', columns=['seq'])\n",
    "\n",
    "df_seq['label'] = df_seq.index.map(lambda x: id2label[x])\n",
    "\n",
    "df_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T01:04:40.925905Z",
     "start_time": "2020-02-03T01:04:40.924256Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=81511991154 % 2**32-1)\n",
    "\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T07:14:17.683606Z",
     "start_time": "2020-02-06T07:14:17.678670Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset15(Dataset):\n",
    "    def __init__(self, df, seq_col, y_col):        \n",
    "#         self.X = df[fea_cols].values\n",
    "        self.X = np.stack(df_seq['seq'].values)\n",
    "#         self.y = pd.get_dummies(df[y_cols]).values\n",
    "        self.y = df[y_col].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx)\n",
    "        return self.X[idx].astype(np.float32), self.y[idx].astype(np.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T07:27:50.855836Z",
     "start_time": "2020-02-06T07:27:50.843483Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, device):\n",
    "        self.device = device\n",
    "        self.model = model#.to(self.device)\n",
    "        self.criterion = criterion#.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        print(self.model.train())\n",
    "        pass\n",
    "    \n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def set_scheduler(self, scheduler):\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def train(self, data_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for data in data_loader:\n",
    "            X_batch, y_batch = data\n",
    "            X_batch = X_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            \n",
    "            y_pred = self.model(X_batch)\n",
    "#             print(y_pred.size(), y_batch.size())\n",
    "            \n",
    "            loss = self.criterion(y_pred, y_batch)\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    def eval(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "#         print('valid_loader', len(valid_loader))\n",
    "        for data in data_loader:\n",
    "            X_batch, y_batch = data\n",
    "            X_batch = X_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                total_loss = total_loss + loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def save(self, model_path='checkpoint.pt'):\n",
    "#         torch.save(self.model.state_dict(), 'checkpoint.pt')\n",
    "        joblib.dump(self.model, model_path)\n",
    "        return\n",
    "    \n",
    "    def load(self, model_path='checkpoint.pt'):\n",
    "#         self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model = joblib.load(model_path)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T08:50:40.679170Z",
     "start_time": "2020-02-06T08:50:40.667554Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, device, num_directions=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers        \n",
    "        self.num_directions = num_directions\n",
    "\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size,\n",
    "                                  hidden_size,\n",
    "                                  num_layers,\n",
    "                                  batch_first=True,\n",
    "                                  bidirectional=(num_directions==2))\n",
    "        \n",
    "        relu = torch.nn.ReLU()\n",
    "        dropout = torch.nn.Dropout(p=0.0001)\n",
    "        \n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * self.num_directions, 198)\n",
    "            \n",
    "#             torch.nn.Linear(hidden_size, 64), \n",
    "#             relu, \n",
    "#             torch.nn.BatchNorm1d(64), \n",
    "#             dropout,\n",
    "#             torch.nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        hidden = Variable(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)).to(self.device)\n",
    "        cell = Variable(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)).to(self.device)\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "#         embed = self.embed(x_seq) # word vector indexing\n",
    "        hidden, cell = self.init_hidden(x_seq.size(0)) # initial hidden,cell\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(x_seq, (hidden, cell))\n",
    "        \n",
    "        # Many-to-Many\n",
    "#         output = self.fc(output) # B,T,H -> B,T,V\n",
    "        \n",
    "        # Many-to-One\n",
    "        hidden = hidden[-self.num_directions:] # (num_directions,B,H)\n",
    "        hidden = torch.cat([h for h in hidden], 1)\n",
    "        output = self.fc(hidden) # last hidden\n",
    "        \n",
    "        return nn.LogSoftmax(dim=1)(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T08:50:41.064624Z",
     "start_time": "2020-02-06T08:50:41.061447Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "print(model_ts)\n",
    "\n",
    "# print(f'fea_size {len(fea_cols)} layer_cols {layer_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T08:50:41.270256Z",
     "start_time": "2020-02-06T08:50:41.266131Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(81511991154)\n",
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T08:50:41.750135Z",
     "start_time": "2020-02-06T08:50:41.747836Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_col = 'seq'\n",
    "y_col = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:02:29.149253Z",
     "start_time": "2020-02-06T09:02:29.004992Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset15(df_seq, seq_col, y_col)\n",
    "\n",
    "train_size = int(df_seq.shape[0] * 0.8)\n",
    "val_size = df_seq.shape[0] - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(len(train_set), len(val_set))\n",
    "\n",
    "batch_size = 25\n",
    "num_workers = 8\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=1)\n",
    "\n",
    "print(f'batch_size {batch_size} num_workers {num_workers}')\n",
    "print(f'train_loader {len(train_loader)} val_loader {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:02:29.163728Z",
     "start_time": "2020-02-06T09:02:29.161394Z"
    }
   },
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:02:29.377124Z",
     "start_time": "2020-02-06T09:02:29.303528Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = DNNModel(input_size=len(fea_cols), dropout_probability=0.7).to(device)\n",
    "# model = CNNModel(dropout_probability=0.7).to(device)\n",
    "model = RNNModel(input_size=input_size, hidden_size=1024, \n",
    "                 num_layers=1, device=device, num_directions=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:02:29.456868Z",
     "start_time": "2020-02-06T09:02:29.454897Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_min = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:02:29.610251Z",
     "start_time": "2020-02-06T09:02:29.608263Z"
    }
   },
   "outputs": [],
   "source": [
    "# criterion(torch.Tensor([[0,0,1]]), torch.Tensor([[0.1,0.2, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:02:29.749241Z",
     "start_time": "2020-02-06T09:02:29.744716Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "# criterion = nn.MultiLabelSoftMarginLoss().to(device)\n",
    "\n",
    "criterion = nn.NLLLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=1.0)\n",
    "\n",
    "trainer = Trainer(model, criterion, optimizer, scheduler, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:02:30.084101Z",
     "start_time": "2020-02-06T09:02:30.081306Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_list = [\n",
    "#     (0.1, 20),\n",
    "    (0.01, 50),\n",
    "    (0.003, 50),\n",
    "    (0.001, 50),\n",
    "    (0.0003, 50),\n",
    "    (0.0001, 50),\n",
    "    (0.00003, 50),\n",
    "    (0.00001, 50),\n",
    "    (0.000005, 50),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:10:40.579919Z",
     "start_time": "2020-02-06T09:02:31.349387Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_epoch = 10000\n",
    "\n",
    "for lr, patience in lr_list:\n",
    "    print(lr, patience)\n",
    "    if os.path.isfile('stop.flag'):\n",
    "        print('stop!')\n",
    "        break\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, min_epoch=1, verbose=True)\n",
    "    early_stopping.val_loss_min = val_loss_min\n",
    "    early_stopping.best_score = None if val_loss_min==np.Inf else -val_loss_min \n",
    "    \n",
    "    trainer.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     trainer.optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#     trainer.optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    trainer.scheduler = StepLR(trainer.optimizer, step_size=50, gamma=1.0)\n",
    "    \n",
    "    for e in tqdm_notebook(range(total_epoch), total=total_epoch, desc='Epoch'):\n",
    "        if os.path.isfile('stop.flag'):\n",
    "            print(f'{e} stop!')\n",
    "            break\n",
    "\n",
    "        train_loss = trainer.train(train_loader)\n",
    "        \n",
    "        if e % 1 == 0:\n",
    "            valid_loss = trainer.eval(val_loader)\n",
    "    #         valid_loss = train_loss\n",
    "\n",
    "            ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "            print(f'[{ts}] Epock {e} / {total_epoch}\\t lr {trainer.scheduler.get_lr()[0]}')\n",
    "            print(f'  train_loss: {train_loss}  valid_loss: {valid_loss}')\n",
    "\n",
    "            early_stopping(valid_loss, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"\\tEarly stopping epoch {}, valid loss {}\".format(e, early_stopping.val_loss_min))\n",
    "                break\n",
    "            \n",
    "\n",
    "    model.load_state_dict(torch.load('model/checkpoint.pt'))\n",
    "#     trainer.load('model/checkpoint.pt')\n",
    "    val_loss_min = early_stopping.val_loss_min\n",
    "    \n",
    "    \n",
    "    model_path = 'model/{}_{}'.format(model_ts, val_loss_min)\n",
    "#     joblib.dump(model, '{}.model'.format(model_path))\n",
    "#     torch.save(model.state_dict(), '{}.pt'.format(model_path))\n",
    "    trainer.save('{}.model'.format(model_path))\n",
    "    print(model_path)\n",
    "\n",
    "    # torch.save(model.state_dict(), f'checkpoint.pt.{train_loss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T09:02:21.257747Z",
     "start_time": "2020-02-06T08:59:50.024Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = joblib.load('model/20200202T232911_0.7551849484443665.model').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T01:06:01.271962Z",
     "start_time": "2020-02-03T01:05:42.664894Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_test = data_loader_all_v2(data_loader_v2, test_list, folder=test_folder, train_label=None, event_time=10, nrows=None)\n",
    "print(test.shape)\n",
    "# test[fea_cols] = scaler.transform(test[fea_cols].values)\n",
    "\n",
    "model.eval()\n",
    "y_pred = model(torch.Tensor(test[fea_cols].values))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T01:06:47.409273Z",
     "start_time": "2020-02-03T01:06:47.391034Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = F.softmax(y_pred)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T01:07:10.383341Z",
     "start_time": "2020-02-03T01:07:10.381038Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_tag ='{}_{}_{}'.format(model_ts, train_loss, valid_loss)\n",
    "model_tag ='{}'.format(model_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T01:07:12.160736Z",
     "start_time": "2020-02-03T01:07:12.022397Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data=y_pred.cpu().detach().numpy())\n",
    "submission.index = test.index\n",
    "submission.index.name = 'id'\n",
    "submission = submission.sort_index()\n",
    "submission = submission.groupby('id').mean()\n",
    "\n",
    "submission.to_csv('submit/{}.csv'.format(model_tag), index=True) \n",
    "model_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T01:07:12.960508Z",
     "start_time": "2020-02-03T01:07:12.931436Z"
    }
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
