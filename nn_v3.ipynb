{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:00:26.594058Z",
     "start_time": "2020-02-08T09:00:24.984878Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# In[1]:\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib \n",
    "import os\n",
    "import glob\n",
    "from konlpy.tag import Mecab\n",
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import gc\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import json\n",
    "from typing import NamedTuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingWarmRestarts\n",
    "print(torch.__version__)\n",
    "# from tools import eval_summary, save_feature_importance, merge_preds\n",
    "from tools import EarlyStopping\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:00:26.596609Z",
     "start_time": "2020-02-08T09:00:26.594969Z"
    }
   },
   "outputs": [],
   "source": [
    "train_folder = 'data/train/'\n",
    "test_folder = 'data/test/'\n",
    "train_label_path = 'data/train_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:00:26.609062Z",
     "start_time": "2020-02-08T09:00:26.597558Z"
    }
   },
   "outputs": [],
   "source": [
    "train_list = os.listdir(train_folder)\n",
    "test_list = os.listdir(test_folder)\n",
    "train_label = pd.read_csv(train_label_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:00:26.613225Z",
     "start_time": "2020-02-08T09:00:26.610061Z"
    }
   },
   "outputs": [],
   "source": [
    "num_class = len(train_label['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:00:26.618854Z",
     "start_time": "2020-02-08T09:00:26.614006Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모든 csv 파일의 상태_B로 변화는 시점이 같다라고 가정\n",
    "# 하지만, 개별 csv파일의 상태_B로 변화는 시점은 상이할 수 있음\n",
    "def data_loader_all_v2(func, files, folder='', train_label=None, event_time=10, nrows=60):   \n",
    "    func_fixed = partial(func, folder=folder, train_label=train_label, event_time=event_time, nrows=nrows)     \n",
    "    if __name__ == '__main__':\n",
    "        pool = Pool(processes=multiprocessing.cpu_count()) \n",
    "        df_list = list(pool.imap(func_fixed, files)) \n",
    "        pool.close()\n",
    "        pool.join()        \n",
    "    combined_df = pd.concat(df_list)    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:00:26.626044Z",
     "start_time": "2020-02-08T09:00:26.620687Z"
    }
   },
   "outputs": [],
   "source": [
    "# train = data_loader_all_v2(data_loader_v2, train_list, folder=train_folder, train_label=train_label, \n",
    "#                            event_time=10, nrows=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:00:37.447373Z",
     "start_time": "2020-02-08T09:00:26.627970Z"
    }
   },
   "outputs": [],
   "source": [
    "train = joblib.load('data/df_train_10_200.pkl').reset_index()\n",
    "test = joblib.load('data/df_test_10.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:00:37.452588Z",
     "start_time": "2020-02-08T09:00:37.448721Z"
    }
   },
   "outputs": [],
   "source": [
    "y_cols = 'label'\n",
    "fea_cols = [c for c in train.columns if c[0] == 'V']\n",
    "len(fea_cols), y_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:01:44.227273Z",
     "start_time": "2020-02-08T09:00:37.453542Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = joblib.load('scaler_20200129T135731.bin')\n",
    "\n",
    "train[fea_cols] = scaler.transform(train[fea_cols].values)\n",
    "test[fea_cols] = scaler.transform(test[fea_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:01:44.231866Z",
     "start_time": "2020-02-08T09:01:44.228327Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=81511991154 % 2**32-1)\n",
    "\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:01:44.243039Z",
     "start_time": "2020-02-08T09:01:44.232631Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset15(Dataset):\n",
    "    def __init__(self, df, fea_cols, y_cols):        \n",
    "        self.X = df[fea_cols].values\n",
    "#         self.y = pd.get_dummies(df[y_cols]).values\n",
    "        self.y = df[y_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].astype(np.float32), self.y[idx].astype(np.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:01:44.251669Z",
     "start_time": "2020-02-08T09:01:44.243834Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, device):\n",
    "        self.device = device\n",
    "        self.model = model#.to(self.device)\n",
    "        self.criterion = criterion#.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        print(self.model.train())\n",
    "        pass\n",
    "    \n",
    "    def train(self, data_loader, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(data_loader):\n",
    "            X_batch, y_batch = data\n",
    "            X_batch = X_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            \n",
    "            self.scheduler.step(epoch + i / len(data_loader))\n",
    "            self.optimizer.zero_grad()\n",
    "            y_pred = self.model(X_batch)\n",
    "#             print(y_pred, y_batch)\n",
    "            \n",
    "            loss = self.criterion(y_pred, y_batch)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    def eval(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "#         print('valid_loader', len(valid_loader))\n",
    "        for data in data_loader:\n",
    "            X_batch, y_batch = data\n",
    "            X_batch = X_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                total_loss = total_loss + loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "\n",
    "    def save(self, model_path='checkpoint.pt'):\n",
    "#         torch.save(self.model.state_dict(), 'checkpoint.pt')\n",
    "        joblib.dump(self.model, model_path)\n",
    "        return\n",
    "    \n",
    "    def load(self, model_path='checkpoint.pt'):\n",
    "#         self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model = joblib.load(model_path)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:01:44.262287Z",
     "start_time": "2020-02-08T09:01:44.252589Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNModel(torch.nn.Module):\n",
    "    def __init__(self, dropout_probability=0.5):\n",
    "        super().__init__()\n",
    "        relu = torch.nn.ReLU()\n",
    "        dropout = torch.nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(1, 2, 3, stride=1, padding=1), torch.nn.BatchNorm1d(2), relu,\n",
    "            torch.nn.Conv1d(2, 4, 3, stride=1, padding=1), torch.nn.BatchNorm1d(4), relu,\n",
    "            torch.nn.MaxPool1d(2),\n",
    "            torch.nn.Conv1d(4, 4, 3, stride=1, padding=1), torch.nn.BatchNorm1d(4), relu,\n",
    "            torch.nn.Conv1d(4, 8, 3, stride=1, padding=1), torch.nn.BatchNorm1d(8), relu,\n",
    "            torch.nn.MaxPool1d(2),\n",
    "            torch.nn.Conv1d(8, 8, 3, stride=1, padding=1), torch.nn.BatchNorm1d(8), relu,\n",
    "            torch.nn.Conv1d(8, 8, 3, stride=1, padding=1), torch.nn.BatchNorm1d(8), relu,\n",
    "            torch.nn.MaxPool1d(2),\n",
    "        )\n",
    "            \n",
    "        self.clf = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5120, 3000), torch.nn.BatchNorm1d(3000), relu, dropout,\n",
    "            torch.nn.Linear(3000, 1024), torch.nn.BatchNorm1d(1024), relu, dropout,\n",
    "            torch.nn.Linear(1024, 198)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        out = self.cnn(x)\n",
    "        dim = 1\n",
    "        for d in out.size()[1:]: #24, 4, 4\n",
    "            dim = dim * d\n",
    "        out = out.view(-1, dim)\n",
    "        out = self.clf(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:01:44.269758Z",
     "start_time": "2020-02-08T09:01:44.263146Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "print(model_ts)\n",
    "\n",
    "# print(f'fea_size {len(fea_cols)} layer_cols {layer_cols}')\n",
    "\n",
    "torch.manual_seed(81511991154)\n",
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:01:46.650817Z",
     "start_time": "2020-02-08T09:01:44.270420Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset15(train[fea_cols + [y_cols]], fea_cols, y_cols)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [int(train.shape[0] * 0.8), int(train.shape[0] * 0.2)])\n",
    "\n",
    "print(len(train_set), len(val_set))\n",
    "\n",
    "batch_size = 3000\n",
    "num_workers = 8\n",
    "\n",
    "all_loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=len(val_set))\n",
    "\n",
    "print(f'batch_size {batch_size} num_workers {num_workers}')\n",
    "print(f'train_loader {len(train_loader)} val_loader {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T12:29:15.242835Z",
     "start_time": "2020-02-08T09:01:46.651594Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_epoch = 10000\n",
    "patience = 200\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, min_epoch=1, verbose=True)\n",
    "early_stopping.val_loss_min = val_loss_min\n",
    "early_stopping.best_score = None if val_loss_min==np.Inf else -val_loss_min \n",
    "\n",
    "model = CNNModel(dropout_probability=0).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "\n",
    "trainer = Trainer(model, criterion, optimizer, scheduler, device)\n",
    "\n",
    "\n",
    "for e in tqdm_notebook(range(total_epoch), total=total_epoch, desc='Epoch'):\n",
    "    if os.path.isfile('stop.flag'):\n",
    "        print(f'{e} stop!')\n",
    "        break\n",
    "\n",
    "    train_loss = trainer.train(all_loader, e)\n",
    "\n",
    "    if e % 1 == 0:\n",
    "#         valid_loss = trainer.eval(val_loader)\n",
    "        valid_loss = train_loss\n",
    "\n",
    "        ts = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "        print(f'[{ts}] Epock {e} / {total_epoch}\\t lr {trainer.scheduler.get_lr()[0]}')\n",
    "        print(f'  train_loss: {train_loss}  valid_loss: {valid_loss}')\n",
    "\n",
    "        early_stopping(valid_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"\\tEarly stopping epoch {}, valid loss {}\".format(e, early_stopping.val_loss_min))\n",
    "            break\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('model/checkpoint.pt'))\n",
    "val_loss_min = early_stopping.val_loss_min\n",
    "\n",
    "model_path = 'model/{}_{}'.format(model_ts, val_loss_min)\n",
    "trainer.save('{}.model'.format(model_path))\n",
    "print(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T12:47:31.175023Z",
     "start_time": "2020-02-08T12:46:37.000751Z"
    }
   },
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "y_pred = model(torch.Tensor(test[fea_cols].values))    \n",
    "\n",
    "y_pred = F.softmax(y_pred)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T12:47:31.216369Z",
     "start_time": "2020-02-08T12:47:31.195056Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tag ='{}cnn_{}_{}'.format(model_ts, train_loss, valid_loss)\n",
    "# model_tag ='{}'.format(model_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T12:47:31.834351Z",
     "start_time": "2020-02-08T12:47:31.222901Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data=y_pred.cpu().detach().numpy())\n",
    "submission.index = test.index\n",
    "submission.index.name = 'id'\n",
    "submission = submission.sort_index()\n",
    "submission = submission.groupby('id').mean()\n",
    "\n",
    "submission.to_csv('submit/{}.csv'.format(model_tag), index=True) \n",
    "model_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T12:47:31.930886Z",
     "start_time": "2020-02-08T12:47:31.837077Z"
    }
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
